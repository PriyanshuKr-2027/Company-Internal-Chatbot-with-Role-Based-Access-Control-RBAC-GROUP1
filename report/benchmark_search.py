"""Quick search benchmark for RAG/RBAC system.
Outputs a markdown summary to report/BENCHMARK.md.
"""
import time
from pathlib import Path
from typing import Dict, List

import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer

PROJECT_ROOT = Path(__file__).resolve().parent.parent
VECTORSTORE_PATH = PROJECT_ROOT / "vectorstore" / "chroma"
OUTPUT_MD = PROJECT_ROOT / "report" / "BENCHMARK.md"

# Sample queries per role
QUERIES: Dict[str, List[str]] = {
    "finance": [
        "What were the financial results for 2024?",
        "Tell me about vendor services expenses",
    ],
    "engineering": [
        "What are the main technical components?",
        "Explain the system architecture",
    ],
    "marketing": [
        "What are the Q4 marketing highlights?",
        "What were the marketing strategies in 2024?",
    ],
    "employee": [
        "What is the remote work policy?",
    ],
}


def run_benchmark():
    client = chromadb.PersistentClient(path=str(VECTORSTORE_PATH))
    collection = client.get_collection(name="company_documents")
    model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

    rows = []
    for role, qs in QUERIES.items():
        role_key = f"role_{role}"
        for query in qs:
            start = time.perf_counter()
            embedding = model.encode(query, normalize_embeddings=False).tolist()
            mid = time.perf_counter()
            results = collection.query(
                query_embeddings=[embedding],
                n_results=3,
                where={role_key: True},
            )
            end = time.perf_counter()

            embed_ms = (mid - start) * 1000
            search_ms = (end - mid) * 1000
            total_ms = (end - start) * 1000

            if results["distances"] and results["distances"][0]:
                top1_distance = results["distances"][0][0]
                relevance = max(0.0, (1 - top1_distance) * 100)
                top1_doc = results["metadatas"][0][0].get("source_document", "?")
            else:
                relevance = 0.0
                top1_doc = "(no result)"

            rows.append(
                {
                    "role": role,
                    "query": query,
                    "embed_ms": round(embed_ms, 2),
                    "search_ms": round(search_ms, 2),
                    "total_ms": round(total_ms, 2),
                    "relevance_pct": round(relevance, 2),
                    "top1": top1_doc,
                }
            )

    write_markdown(rows)


def write_markdown(rows):
    lines = []
    lines.append("# Search Benchmark\n")
    lines.append("_Auto-generated by report/benchmark_search.py_\n")
    lines.append("\n")
    lines.append("## Summary\n")

    # Simple aggregates
    if rows:
        avg_total = sum(r["total_ms"] for r in rows) / len(rows)
        avg_relevance = sum(r["relevance_pct"] for r in rows) / len(rows)
        lines.append(f"- Avg latency (encode + search): {avg_total:.2f} ms")
        lines.append(f"- Avg top-1 relevance: {avg_relevance:.2f}%")
    lines.append("\n")

    lines.append("## Per-Query Results\n")
    lines.append("| Role | Query | Latency (ms) | Top-1 Relevance | Top-1 Source |\n")
    lines.append("|------|-------|--------------|-----------------|--------------|\n")
    for r in rows:
        lines.append(
            "| "
            f"{r['role']} | "
            f"{r['query']} | "
            f"{r['total_ms']} (embed {r['embed_ms']}, search {r['search_ms']}) | "
            f"{r['relevance_pct']}% | "
            f"{r['top1']} |"
        )

    OUTPUT_MD.write_text("\n".join(lines), encoding="utf-8")
    print(f"Benchmark written to {OUTPUT_MD}")


if __name__ == "__main__":
    run_benchmark()
